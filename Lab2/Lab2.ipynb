{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Burst Photography\n",
    "\n",
    "This lab uses sequences of images to composite a new image or to extract some additional information.\n",
    "\n",
    "Remember you can do aritmetics with images directly when they are loaded as numpy arrays. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#some magic to show the images inside the notebook\n",
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import imageio as imio\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# A hepler function for displaying images within the notebook.\n",
    "# It displays an image, optionally applies zoom the image.\n",
    "def show_image(img, zoom=1.5):\n",
    "    dpi = 77\n",
    "    plt.figure(figsize=(img.shape[0]*zoom/dpi,img.shape[0]*zoom/dpi))\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.repeat(img[:,:,np.newaxis],3,2)        \n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    \n",
    "\n",
    "# A hepler function for displaying images within the notebook.\n",
    "# It may display multiple images side by side, optionally apply gamma transform, and zoom the image.\n",
    "def show_images(imglist, zoom=1, needs_encoding=False):\n",
    "    if type(imglist) is not list:\n",
    "       imglist = [imglist]\n",
    "    n = len(imglist)\n",
    "    first_img = imglist[0]\n",
    "    dpi = 77 # pyplot default?\n",
    "    plt.figure(figsize=(first_img.shape[0]*zoom*n/dpi,first_img.shape[0]*zoom*n/dpi))\n",
    "    for i in range(0,n):\n",
    "        img = imglist[i]\n",
    "        plt.subplot(1,n,i + 1)\n",
    "        plt.tight_layout()    \n",
    "        plt.axis('off')\n",
    "        if len(img.shape) == 2:\n",
    "           img = np.repeat(img[:,:,np.newaxis],3,2)\n",
    "        plt.imshow(img, interpolation='nearest')    \n",
    "    \n",
    "\n",
    "def saveHDR(filename, image):\n",
    "    f = open(filename, \"wb\")\n",
    "    f.write(str.encode(\"#?RADIANCE\\n# Made with Python & Numpy\\nFORMAT=32-bit_rle_rgbe\\n\\n\"))\n",
    "    f.write(str.encode(\"-Y {0} +X {1}\\n\".format(image.shape[0], image.shape[1])))\n",
    "    \n",
    "    brightest = np.maximum(np.maximum(image[...,0], image[...,1]), image[...,2])\n",
    "    mantissa = np.zeros_like(brightest)\n",
    "    exponent = np.zeros_like(brightest)\n",
    "    np.frexp(brightest, mantissa, exponent)\n",
    "    scaled_mantissa = mantissa * 256.0 / brightest\n",
    "    rgbe = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
    "    rgbe[...,0:3] = np.around(image[...,0:3] * scaled_mantissa[...,None])\n",
    "    rgbe[...,3] = np.around(exponent + 128)\n",
    "    \n",
    "    rgbe.flatten().tofile(f)\n",
    "    f.close()\n",
    "\n",
    "def dcraw_meta(path):\n",
    "    lines = subprocess.check_output([\"dcraw\", \"-i\", \"-v\", path]).decode(\"utf-8\").splitlines()\n",
    "    lines = [[x.strip() for x in line.split(\":\", 1)] for line in lines if \":\" in line]\n",
    "    lines = { x[0] : x[1] for x in lines }\n",
    "\n",
    "    if \"Aperture\" in lines:\n",
    "        lines[\"Aperture\"] = float(lines[\"Aperture\"].split(\"/\")[1])\n",
    "\n",
    "    if \"ISO speed\" in lines:\n",
    "        lines[\"ISO speed\"] = float(lines[\"ISO speed\"])\n",
    "\n",
    "    if \"Shutter\" in lines:\n",
    "        shutter = lines[\"Shutter\"].split()[0]\n",
    "        shutter = shutter.split(\"/\")\n",
    "        lines[\"Shutter\"] = float(shutter[0]) / float(shutter[1])\n",
    "\n",
    "    return lines\n",
    "\n",
    "def getExposureInSeconds(path):\n",
    "    meta = dcraw_meta(path)\n",
    "    result = meta[\"Aperture\"] ** 2 / (meta[\"Shutter\"] * meta[\"ISO speed\"])\n",
    "    return meta[\"Shutter\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 1: HDR (20)\n",
    "\n",
    "You are given a sequence of images in RAW format. Use dcraw to convert them to a linear 16 bits\n",
    "tiff. Merge these images using the formula from Robertson’s method (below). In order to load these images, you can use imageio, which works the same as ndimage, but supports tiff 16 and HDR images. (Note that values are not between 0 and 255 in this case). \n",
    "\n",
    "You need to get the exposure time of your images from the EXIF metadata. You can do this using DCRAW, you are given a function for this.\n",
    "\n",
    "Robertson’s method is actually more complicated, but for us, we will simply apply this weighting. Normalize the values between 0 and 1 before doing operations on them.\n",
    "Save the result an HDR image. Save it as an EXR image or HDR format. You are given some code\n",
    "to save the file in HDR format. You can try to use OpenEXR bindings in python, OpenCV also supports EXR.\n",
    "\n",
    "\n",
    "Robertson's: ![Alt](./hdr7.png \"Title\")\n",
    "\n",
    "Individual exposure (image value $I_{linear}$) is radiance (X) * exposure time (t):\n",
    "\n",
    "$I_{lin} = tX$\n",
    "\n",
    "### Memorial Sequence\n",
    "\n",
    "You have a sequence of the Memorial Dataset. We do not have the camera response curve for this dataset, so the merging will probably suffer some artefacts. You can approximate the camera response curve with a gamma correction.\n",
    "As an extra exercise, you can take the code from the last cell in this jupyter notebook to work with this sequence so you estimate the camera response curve.\n",
    "\n",
    "\n",
    "### Opening your HDR images. \n",
    "HDR formats start to be supported by operative systems, but typically you will see a tonemapped version of the image. A nice tool is the Luminance software, http://qtpfsgui.sourceforge.net/?page_id=10 which allows you to open and modify HDR images. It also includes several tonemapping opperators.\n",
    "\n",
    "### Take your own sequence (If you can)\n",
    "\n",
    "You will need a tripod and a camera where you can change the exposure time. Set it to bracket more with the longer number of exposures possible. \n",
    "\n",
    "### Display radiance (luminance) in false color (blue to red)\n",
    "\n",
    "https://matplotlib.org/users/image_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Tone Mapping (40)\n",
    "\n",
    "We will do a simple implementation of the Bilateral Filter Tone Mapping algorithm seen in class.\n",
    "http://people.csail.mit.edu/fredo/PUBLI/Siggraph2002/\n",
    "You can use the opencv bilateral filter implementation. You can also find details on the gradient based tone mapping commented in class here: http://www.cs.huji.ac.il/~danix/hdr/hdrc.pdf\n",
    "\n",
    "A more comprehensive list of tone mapping algorithms can be found [here](https://www.google.pl/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&cad=rja&uact=8&ved=0ahUKEwjr_OGauo7XAhXK6xoKHXG5AhQQFghAMAM&url=http%3A%2F%2Fcadik.posvete.cz%2Ftmo%2F&usg=AOvVaw0kU5sdvkmnR5zky_4EQDCz). \n",
    "\n",
    "### This method\n",
    "\n",
    "The basic idea is to get intensity of the image, take its log, and apply the bilateral filter. Then divide\n",
    "the log(intensity) by the filtered image and do some contrast correction. Then, take the inverse of\n",
    "the log, and multiply by the color ratios.\n",
    "\n",
    "Here is the high-level set of operation that you need to do in order to perform contrast reduction\n",
    "\n",
    "```python\n",
    "#pseudocode\n",
    "input intensity= 1/61(R*20+G*40+B)\n",
    "r=R/(input intensity), g=G/input intensity, B=B/input intensity\n",
    "log(base)=Bilateral(log(input intensity))\n",
    "log(detail)=log(input intensity)-log(base)\n",
    "log (output intensity)=log(base)compressionFactor+log(detail) - log_absolute_scale\n",
    "R output = r*10^(log(output intensity)), etc.\n",
    "```\n",
    "\n",
    "#### Note from the authors\n",
    "You can replace the first formula by your favorite intensity expression.\n",
    "You can replace the multiplication by compressionfactor by your favorite contrast-reduction curve\n",
    "(e.g. histogram adjustment, Reinhard et al.'s saturation function, etc.)\n",
    "\n",
    "compressionfactor and log_absolute_scale are image-dependent.\n",
    "\n",
    "compressionfactor makes sure the contrast matches a given target contrast. It is the ratio of this target and the contrast in the base layer:\n",
    "\n",
    "```python\n",
    "#pseudocode\n",
    "targetContrast/(max(log(base)) - min(log(base)))\n",
    "```\n",
    "I use log(5) as my target, but you can use a bigger value to preserve more contrast.\n",
    "log_absolute_scale essentially normalizes the image, by making sure that the biggest value in the\n",
    "base after compression is 1. \n",
    "\n",
    "It is equal to $max(log(base))*compressionfactor$\n",
    "All log are in base 10.\n",
    "\n",
    "\n",
    "Use Luminance HDR software to load and compare the results with yours. Explore the different\n",
    "tone mappers and their differences. You can also use this software to create, load and compare\n",
    "your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Stack\n",
    "\n",
    "If you try to capture your own sequence, you will notice that there is a small change in magnification that occurs as you change the focus. This is due to the change of focal length that happens when adjusting focus.\n",
    "These sequences need to be compensated for this small change in magnification.\n",
    "\n",
    "I did this with the sequence fs1 using Hugin (http://hugin.sourceforge.net/) software, there is a sub-application for aligning images. Here a tutorial https://patdavid.net/2013/01/focus-stacking-macro-photos-enfuse.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: All in Focus Image (20)\n",
    "\n",
    "Compute an all-in-focus image from a focal stack. Images of a focal stack are provided with focuses at different focal planes.\n",
    "\n",
    "I would recommend you to start with the \"sim\" sequence. The fs1 sequence has not been tested. The other sequences come from this paper, which uses a more sophisticated approach [Moeller 2015](https://github.com/adrelino/variational-depth-from-focus). Here another paper on the topic if you want to learn more [Jacobs et al. 2012](https://graphics.stanford.edu/papers/focalstack/)\n",
    "\n",
    "The idea is to estimate for each pixel in which image is more in focus. This will only work on areas where there is texture/detail. Two simple approaches are to use the first derivative or the second derivative of a gaussian (Laplacian) to measure the sharpness of the image in a particular pixel. You should use a single channel (or combination of them), rather than the 3 channels separately. \n",
    "\n",
    "Fuse all three images by picking the pixel value with the highest gradient magnitude or bigger response to the Laplacian among all images. Plot an false color image with the image contributions. \n",
    "\n",
    "Computing Gradient: \n",
    "You can have a look to ndimage.filters. You can try gaussian filters with first and second derivatives and different sigmas, the laplacian, and gradient magnitude, and obseve the results.  \n",
    "\n",
    "Also, you can define your own kernel and use convolutions with a simple 3x3 kernel.\n",
    "\n",
    "https://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Depth from Focus (20)\n",
    "\n",
    "Using the same method, create a depth image assigning depths to the pixels depending on their focal plane. This will be blocky/noisy. You can check with some of the filters that we used so far (Gaussian/Median/Bilateral) to improve the depth map. Display the depthmap using a false color scheme (blue to red). \n",
    "\n",
    "https://matplotlib.org/users/image_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA: Curve Response Estimation\n",
    "\n",
    "This is some code translation from the [Devebec and Malik](http://www.pauldebevec.com/Research/HDR/debevec-siggraph97.pdf) paper on HDR imaging. The paper has some Matlab code at the end, if you find some bug in my code, let me know. You can adapt it to work with more images to make it work with the Memorial dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w(z):\n",
    "    return 128. - abs(z-128.)\n",
    "\n",
    "def gsolve(Z,B,l):\n",
    "    n = 256 # ?\n",
    "    \n",
    "    print (Z.shape[1])\n",
    "    \n",
    "    A = np.zeros((Z.shape[1]*Z.shape[0]+n+1, n + Z.shape[1]))\n",
    "    b = np.zeros((A.shape[0],1))\n",
    "    \n",
    "    print (A.shape)    \n",
    "    print (b.shape)\n",
    "    print (Z.shape[1])\n",
    "    print (Z.shape[0])\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(Z.shape[1]):\n",
    "        for j in range(Z.shape[0]):\n",
    "            wij = w(Z[j,i]+1)\n",
    "            A[k, Z[j,i]] = wij\n",
    "            A[k, n+i] = -wij\n",
    "            b[k] = wij * B[j]\n",
    "            k = k+1\n",
    "    \n",
    "    A[k,127] = 1.\n",
    "    k = k+1\n",
    "    \n",
    "    #print (A)\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        A[k,i] = l*w(i+2)\n",
    "        A[k,i+1] = -2.*l*w(i+2)\n",
    "        A[k,i+2] = l*w(i+2)\n",
    "        k=k+1\n",
    "    \n",
    "    x = np.linalg.lstsq(A, b)[0]\n",
    "    #print (x.shape)\n",
    "    g = x[0:n]\n",
    "    lE = x[n:x.shape[0]]\n",
    "    return (g,lE)\n",
    "        \n",
    "\n",
    "def debevecMalik(probe0,probe1,probe2,output,B, l):\n",
    "    #get pixesl for Z\n",
    "    result = np.zeros(probe0.shape)\n",
    "    gamma = range(256)\n",
    "    n = 256 # maybe not\n",
    "    gamma = np.power(np.divide(gamma,255),2.2)*8.\n",
    "    #samples some pixels\n",
    "    ys = np.random.randint(probe0.shape[0], size=(50))\n",
    "    xs = np.random.randint(probe0.shape[0], size=(50))   \n",
    "    \n",
    "    for c in range(3):\n",
    "        samples = []\n",
    "        ss = 0\n",
    "        for y,x in zip(ys,xs):\n",
    "            check_under = probe0[y,x,c]\n",
    "            check_over  = probe2[y,x,c]\n",
    "            if (check_under < 1. or check_over > 254):\n",
    "                ss = ss +1\n",
    "            samples.append((y,x,c))\n",
    "        \n",
    "        print (\"Saturated samples \", ss)\n",
    "        Z = np.zeros((3,len(samples))).astype(int)\n",
    "        for i,s in enumerate(samples):\n",
    "            Z[0,i] = float(probe0[s])\n",
    "            Z[1,i] = float(probe1[s])\n",
    "            Z[2,i] = float(probe2[s])\n",
    "        \n",
    "        g, lE = gsolve(Z,B,l)\n",
    "              \n",
    "        if c == 1:\n",
    "            plot(g,range(n))\n",
    "\n",
    "        for y in range(probe1.shape[0]):\n",
    "            for x in range(probe1.shape[1]):\n",
    "                z0 = probe0[y,x,c]\n",
    "                z1 = probe1[y,x,c]\n",
    "                z2 = probe2[y,x,c]\n",
    "                value = w(z0)*(g[z0] - B[0]) + w(z1)*(g[z1] - B[1]) + w(z2)*(g[z2] - B[2])\n",
    "                ws = w(z0)+w(z1)+w(z2)\n",
    "                result[y,x,c] = math.exp(value/ws)\n",
    "                \n",
    "    #print (g)\n",
    "    saveHDR(output, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
